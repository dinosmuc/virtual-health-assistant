{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e424f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyboard\n",
    "import os\n",
    "import tempfile\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import openai\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "from elevenlabs import generate, play, set_api_key\n",
    "from IPython.display import Audio, display\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62bfa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '<>'\n",
    "openai.api_key = api_key\n",
    "set_api_key(\"<>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56a584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 48000  # sample rate\n",
    "channels = 1  # number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4197c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(fs,cannels):\n",
    "    print(\"Press and hold spacebar to start recording. Release to stop.\")\n",
    "    while True:\n",
    "        if keyboard.is_pressed('space'):\n",
    "            print(\"Recording...\")\n",
    "            recorded_chunks = []\n",
    "            with sd.InputStream(samplerate=fs, channels=channels) as stream:\n",
    "                while keyboard.is_pressed('space'):\n",
    "                    audio_chunk, overflowed = stream.read(fs)\n",
    "                    recorded_chunks.append(audio_chunk)\n",
    "            recording = np.concatenate(recorded_chunks, axis=0)\n",
    "            print(\"Finished recording.\")\n",
    "            return recording\n",
    "        sd.sleep(100)  # Sleep for a short duration before checking again\n",
    "\n",
    "def transcribe_audio(recording, fs):\n",
    "    # This function is kept for completeness. You might not need this if you're not transcribing.\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio:\n",
    "        sf.write(temp_audio.name, recording, fs)\n",
    "        # Transcription part should go here if needed\n",
    "        temp_audio.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f2995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(recording, fs):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_audio:\n",
    "        sf.write(temp_audio.name, recording, fs)\n",
    "        temp_audio.close()\n",
    "        with open(temp_audio.name, \"rb\") as audio_file:\n",
    "            transcript = openai.Audio.transcribe(model=\"whisper-1\",\n",
    "                                                 file=audio_file,\n",
    "                                                 language=\"en\")\n",
    "        os.remove(temp_audio.name)\n",
    "    return transcript[\"text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0a3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_generated_audio_doctor(text, voice=\"Callum\", model=\"eleven_monolingual_v1\"):\n",
    "    audio = generate(text=text, voice=voice, model=model)\n",
    "    display(Audio(data=audio, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad5c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_generated_audio_nurse(text, voice=\"Charlotte\", model=\"eleven_monolingual_v1\"):\n",
    "    audio = generate(text=text, voice=voice, model=model)\n",
    "    display(Audio(data=audio, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c09db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0,\n",
    "                   model_name=\"gpt-4\",\n",
    "                   max_tokens=200,\n",
    "                   openai_api_key=api_key,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa93b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1 = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6054ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2 = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "981686d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"\n",
    "As a virtual nurse, your objective is to gather the following patient details:\n",
    "- Name\n",
    "- Reason for Call\n",
    "- Severity of Symptoms\n",
    "- Current Medications\n",
    "- Known Allergies\n",
    "- Recent Medical Events\n",
    "\n",
    "Engage the patient in a relaxed conversation. Pose your questions one by one, ensuring you await a response after each question. Feel free to indulge in casual chat for a comfortable environment. \n",
    "\n",
    "Once you've collected all the information, wrap up the chat gracefully and conect patient directly to doctor.\n",
    "\n",
    "{chat_history}\n",
    "Patient: {human_input}\n",
    "Virtual Nurse: \n",
    "\"\"\"\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=template1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc8c0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\"\n",
    "You are a Virtual Health Assistant. Your role is to provide health information, guidance, and active listening. Engage the patient in a natural, conversational manner. Ask follow-up questions one at a time to delve deeper into the patient's condition. Once you've gathered sufficient information, provide your final assessment and diagnosis. Remember to mention that this is solely for informational purposes and does not replace a professional medical diagnosis. Recommend over-the-counter solutions if appropriate, but if the patient's condition seems to require professional medical attention, guide them to consult with a real doctor. Start by introducing yourself and proceed with a gentle, inquiry-based approach to understand the patient's situation better.\n",
    "{chat_history}\n",
    "Patient: {human_input}\n",
    "Virtual Health Assistant: \n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"],\n",
    "    template=template2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56b00fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template3 = \"\"\"\n",
    "Create a concise summary from the patient and nurse conversation, including the patient's name, reason for call, severity of symptoms, current medications, known allergies, and recent medical events.\n",
    "Patient and Nurse conversation: {data}\n",
    "Output: \"\"\"\n",
    "\n",
    "prompt3 = PromptTemplate(\n",
    "    input_variables=[\"data\"],\n",
    "    template=template3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92e5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_nurse=llm_chain = LLMChain(llm=model,\n",
    "                                 prompt=prompt1,\n",
    "                                 verbose=True,\n",
    "                                 memory=memory1\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ecde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_doctor=llm_chain = LLMChain(llm=model,\n",
    "                             prompt=prompt2,\n",
    "                             verbose=True,\n",
    "                             memory=memory2\n",
    "                            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_data=llm_chain = LLMChain(llm=model,\n",
    "                             prompt=prompt3,\n",
    "                             verbose=True,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_to_string(memory_object):\n",
    "    # Convert the object to string representation\n",
    "    string_representation = str(memory_object)\n",
    "    \n",
    "    # Extracting the message contents\n",
    "    human_messages = re.findall(r\"HumanMessage\\(content='(.*?)'\", string_representation)\n",
    "    ai_messages = re.findall(r\"AIMessage\\(content=\\\"(.*?)\\\"\", string_representation)\n",
    "    \n",
    "    # Creating a conversation format\n",
    "    conversation = \"\"\n",
    "    for human, ai in zip(human_messages, ai_messages):\n",
    "        conversation += \"Human: \" + human + \"\\n\"\n",
    "        conversation += \"AI: \" + ai + \"\\n\"\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36734a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "exit_loops = False\n",
    "\n",
    "response = chain_nurse.predict(human_input=\"Hi\")\n",
    "play_generated_audio_nurse(response)\n",
    "while True:\n",
    "    \n",
    "    keyboard.wait(\"space\")  # wait for spacebar to be pressed\n",
    "    recorded_audio = record_audio(fs, channels) \n",
    "    message = transcribe_audio(recorded_audio, fs)\n",
    "    print(f\"You: {message}\")\n",
    "    response = chain_nurse.predict(human_input=message)\n",
    "    play_generated_audio_nurse(response)\n",
    "    context = memory_to_string(memory1)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if \"doctor\" in context.lower():\n",
    "        print(\"Context:\", context)\n",
    "        data = chain_data.predict(data=context)\n",
    "        print(\"Model format output :\", data)\n",
    "        response = chain_doctor.predict(human_input=data)\n",
    "        play_generated_audio_doctor(response)\n",
    "        while True:\n",
    "            keyboard.wait(\"space\")  # wait for spacebar to be pressed\n",
    "            recorded_audio = record_audio(fs, channels) \n",
    "            message = transcribe_audio(recorded_audio, fs)\n",
    "\n",
    "            print(f\"You: {message}\")\n",
    "\n",
    "\n",
    "            response = chain_doctor.predict(human_input=message)\n",
    "\n",
    "            play_generated_audio_doctor(response)\n",
    "            \n",
    "            \n",
    "            if \"bye\" in context:\n",
    "                exit_loops = True\n",
    "                break\n",
    "    \n",
    "    if exit_loops == True:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "langchain_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
